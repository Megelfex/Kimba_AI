# core/llm_router.py

from llama_cpp import Llama

class KimbaLLM:
    """
    EN: Wrapper for initializing and querying a local LLM (e.g. GGUF model via llama.cpp).
    DE: Wrapper zur Initialisierung und Abfrage eines lokalen LLMs (z.â€¯B. GGUF-Modell via llama.cpp).
    """

    def __init__(self, model: str = "models/openhermes-2.5-mistral-7b.Q5_K_M.gguf", mode="auto"):
        """
        EN: Initializes the LLM with specified model path and configuration.
        DE: Initialisiert das Sprachmodell mit angegebenem Pfad und Konfiguration.

        Args:
            model (str): Path to the local GGUF model file.
            mode (str): Optional logic mode (not yet implemented).
        """
        print("ðŸ§© LLM wird geladen ...")
        self.model = Llama(
            model_path=model,
            n_ctx=2048,       # EN: Context size â€“ DE: Kontextfenster
            n_threads=8,      # EN: Adjust for your CPU â€“ DE: Je nach CPU anpassen
            n_gpu_layers=0,   # EN: GPU acceleration, if any â€“ DE: GPU-Nutzung, falls vorhanden
            verbose=True
        )
        print("âœ… LLM erfolgreich geladen!")

    def ask(self, prompt):
        """
        EN: Sends a prompt to the LLM and returns the generated response.
        DE: Sendet einen Prompt an das Modell und gibt die generierte Antwort zurÃ¼ck.

        Args:
            prompt (str): The input text or question to ask the model.

        Returns:
            str: The text response generated by the LLM.
        """
        prompt_template = f"[INST] {prompt.strip()} [/INST]"
        response = self.model(prompt_template, max_tokens=512, stop=["</s>"])
        return response["choices"][0]["text"].strip()
